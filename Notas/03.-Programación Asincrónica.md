# Tareas Asincrónicas

## 1.- La Idea

Uno de los principales problemas de los threads es que consumen muchos recursos: como cada uno tiene su propio stack, hay mucho overhead de creación, uso de memoria, y destrucción. No es un modelo muy escalable.

Para solucionar ese problema, se implementan **_tareas asincrónicas_**. Son un conjunto de tareas que corren de forma intercalada en una pool de threads. Son mucho más livianas y eficientes que los threads y tienen mucho menos overhead, por lo que se pueden tener muchas más en un único programa.

El código resultante es parecido al de threads, salvo que las operaciones bloqueantes se manejan de forma distinta. Veamos el siguiente código de ejemplo, en versión _sincrónica_ con threads:

```Rust
use std::{net, thread};
let listener = net::TcpListener::bind(address)?;

for socket_result in listener.incoming() {
    let socket = socket_result?;
    let groups = chat_group_table.clone();
    thread::spawn(|| {
        log_error(serve(socket, groups));
    });
}
```

Este es un código sencillo que escucha conexiones en un socket y crea un thread por cada conexión nueva para unirla a un chat. El problema es que genera un nuevo thread por cliente, y si tenemos muchos clientes podríamos tener problemas de performance. La versión async sería:

```Rust
use async_std::{net, task};
let listener = net::TcpListener::bind(address).await?;

let mut new_connections = listener.incoming();
while let Some(socket_result) = new_connections.next().await
    let socket = socket_result?;
    let groups = chat_group_table.clone();
    task::spawn(async {
        log_error(serve(socket, groups).await);
    });
```

La única diferencia visible conb la otre versión es que cada cliente crea una `task` en lugar de un `thread`, que algunas llamadas tienen un await al final, que hay un async en el spawn, y que en vez de `std::net` se usa `async_std::net`. A nivel de ejecución son bastante distintas. Veremos algunos conceptos para poder explicar la diferencia.

## 2.- Futures

Hay tareas que toman tiempo de espera. Siguiendo el ejemplo anterior, las conexiones de red requeren de varias _syscalls_, además de tener que esperar por la respuesta del otro lado de la conexión. Sería bueno que el thread pueda tomar otras tareas mientras espera la ejecución de otras. Para casos como esos, Rust implementa el trait `std::future::Future`:

```Rust
trait Future {
    type Output;
    // por ahora, interpretar ‘Pin<&mut Self>‘
    // como ‘&mut Self‘.
    fn poll(self: Pin<&mut Self>, cx: &mut Context<’_>)
    -> Poll<Self::Output>;
}

enum Poll<T> {
    Ready(T),
    Pending,
}
```

Un future representa una operación sobre la que se puede testear si se terminó o no. A bajo nivel, el OS implementa syscalls que ayudan a verificar el estado de ciertas tareas. El método `poll` _nunca es bloqueante_, siempre retorna inmediatamente una de las variantes: o el resultado de la operación, o `Pending`. Esto es el **_modelo piñata_** de la programación asicrónica: le pego a la operación hasta que caiga un resultado.

Cada vez que se llama a poll, el future va a avanzar todo lo posible en su ejecución hasta que llegue a una operación bloqueante. el Future almacena todo el contexto necesario para poder reanudar la ejecución en cualquier momento. Rust implementa su arquitectura async de forma eficiente: sólo se llama a poll cuando puede avanzar o cuando puede devolver un resultado.

El tipo de dato `Pin` es una variable que _no cambia de posición en memoria_. En un programa sincrónico nos puede interesar dejarle al compilador la libertad de realocar objetos a distintas posiciones para mejorar la eficiencia, pero como los futures necesitan guardar _contexto_ entre ejecuciones intercaladas por otro código, es importante que la memoria involucrada no se mueva para evitar referencias inválidas.

## 3.- Async/Await

Ahora necesitamos una manera de crear y llamar futures. Para eso, Rust implementa las expresiones `async` y `await`. Trabajemos sobre el siguiente código de ejemplo:

```Rust
use async_std::io::prelude::*;
use async_std::net;

async fn cheapo_request(host: &str, port: u16, path: &str)
    -> std::io::Result<String>
{
    let mut socket =
        net::TcpStream::connect((host, port)).await?;
    let request =
        format!("GET {} HTTP/1.1\r\nHost: {}\r\n\r\n", path, host)
    socket.write_all(request.as_bytes()).await?;
    socket.shutdown(net::Shutdown::Write)?;
    let mut response = String::new();
    socket.read_to_string(&mut response).await?;
    Ok(response)
}
```

Las funciones que se marcan con `async` retornan inmediatamente cuando se llaman, no se ejecuta el cuerpo. Devuelven un `Future` con todo el contexto necesario (el compilador maneja los tipos de las funciones acordemente). Cuando se llama el primer `poll` sobre el retorno del async, la función empieza a ejecutarse hasta que llega al primer `await` (en nuestro caso, el `connect`). Si no se completó, retorna `Pending` y la función devuelve el estado actual (estado local y el punto donde debe retomarse la ejecución).

El await tiene que tomar ownership del future para hacer poll (necesita el contexto). Si el poll retorna `Ready` el resultado de ese future es el valor que devuelve el await y continúa la ejecución. Sino, retorna pending a la expresión que lo invocó. Como el await requiere la capacidad de detenerse y continuar, solo se puede usar con expresiones _async_.

La segunda llamada a `poll` sobre `cheapo_request` continuará desde el `connect`. Si ese await devuelve pending, `cheapo_request` devuelve pending también. Si devuelve `Ready`, toma el valor del socket y continúa hasta el siguiente await (`write_all`). Llamadas sucesivas siguen el mismo proceso hasta que la función llega al final o a un error, y puede devolver un `Ready<Result<String>>`.

## 4.- Block on

Ahora necesitamos formas de integrar funciones asincrónicas en nuestro programa. El caso simple es que necesitemos esperar de forma _sincrónica_ la finalización de una tarea _asincrónica_. Para eso existe la función `block_on`, que devuelve el resultado de la tarea asincrónica:

```Rust
fn main() -> std::io::Result<()> {
    use async_std::task;
    let response =
        task::block_on(cheapo_request("example.com",80,"/"))?;
    println!("{}", response);
    Ok(())
}
```

Este modelo de concurrencia se conoce como _concurrencia colaborativa_, ya que distintos threads colaboran para ejecutar tareas bloqueantes sin acaparar el procesador. `block_on` sabe por cuánto tiempo debería hacer sleep antes de llamar a `poll` de nuevo, dejando que se ejecuten otros hilos del programa principal.

## 5.- Spawn

También puede darse el caso de que necesitemos ejecutar de forma _asincrónica_ varias tareas que a su vez son _asincrónicas_ entre sí. Para eso existe la familia de funciones `spawn` que son análogas al spawn de los threads.

`spawn` recibe un future y agrega una tarea a la pool de threads dedicados a hacer los `poll`. El poll puede ser llamado en cualquier momento y desde cualquier thread, lo que podría generar problemas. Para eso existe la llamada `spawn_local`, que agrega el future a una pool que sólo hace polling en el `block_on` del thread en el que se llamó. También existe `spawn_blocking` que agrega la task a una pool dedicada a tareas de cómputo pesado (para operaciones del estilo transformada de fourier que son de mucho cálculo y pocas operaciones bloqueantes). Devuelven handles para poder esperar a las tareas después (con `block_on` o `await`)

```Rust
fn main() -> std::io::Result<()> {
    use async_std::task;
    let task1 =
        task::spawn(cheapo_request("example.com",80,"/"))?;
    let task2 =
        task::spawn(cheapo_request("testing.com",80,"/"))?;

    let response1 = task::block_on(task1);
    let response2 = task::block_on(task2);

    println!("{}", response1);
    println!("{}", response2);
    Ok(())
}
```

Existen otras formas de crear y esperar múltiples futures a la vez, pero no se entrará más a detalle en esta clase. Como referencia está el [manual de porgramación asincrónica de Rust](https://rust-lang.github.io/async-book/06_multiple_futures/01_chapter.html).

## 6.- Executors y yielding

Los distintos tipos de `spawn` existen debido a la diferencia en el _"scheduling"_ entre las tareas async y los threads. La ejecución de los threads la maneja el scheduler del OS: los aloja y los desaloja cuando lo vea conveniente, y las librerías de threads son wrappers de la api de threads que provee el OS con algunas funcionalidades agregadas. En cambio, las tareas async en Rust _pueden correr en varios threads o no_, y la ejecución de las tareas la maneja la propia librería de Rust. Esto tiene la ventaja de que los _"cambios de contexto"_ entre tareas pueden no ser cambios de contexto reales, sino simplemente ejecución de código (la librería de `async_std`) en un mismo contexto. `block_on` y `spawn` son lo que se denomina _executors_: se encargan de la ejecución de las tasks.

Como las tareas en Rust las maneja Rust y no el OS, hay que explicitar cuándo queremos que una tarea sea "desalojada" para darle oportunidad a otras tareas de ejecutarse. Para eso existen funciones como `yield_now`: podemos agregar la línea `async_std::task::yield_now().await` en cualquier parte de una función asincrónica, y el await devolverá `Pending` la primera vez y `Ready` la segunda. Por ejemplo:

```Rust
use async_std::io::prelude::*;
use async_std::net;

async fn process_response(response: String)
    -> std::io::Result<String>
{
    let aux = very_cpu_intensive_operation(response);
    async_std::task::yield_now().await?;
    let result = another_very_cpu_intensive_operation(aux);

    Ok(result)
}

fn main() -> std::io::Result<()> {
    use async_std::task;
    let response =
        task::block_on(cheapo_request("example.com",80,"/"))?;
    let result =
        task::block_on(process_response(response));
    println!("{}", result);
    Ok(())
}
```
